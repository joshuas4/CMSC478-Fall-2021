{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>CMSC 478: Machine Learning</center></h1>\n",
    "\n",
    "<center><img src=\"img/title.jpg\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "<h3 style=\"color:blue;\"><center>Instructor: Fereydoon Vafaei</center></h3>\n",
    "\n",
    "\n",
    "<h5 style=\"color:purple;\"><center>Support Vector Machines (SVM)</center></h5>\n",
    "\n",
    "<center><img src=\"img/UMBC_logo.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Agenda</center></h1>\n",
    "\n",
    "- <b>Support Vector Machines:</b>\n",
    "    - Binary Classification and Linear Separability\n",
    "    \n",
    "    - Linear Classifiers and Perceptron Algorithm\n",
    "\n",
    "    - Hard Margin vs Soft Margin SVM\n",
    "\n",
    "    - SVM Hyperparameters\n",
    "\n",
    "    - SVM Optimization Problem\n",
    "        - SVM Cost Function\n",
    "        - Convex and Non-convex Cost Functions\n",
    "        - Hinge Loss Function\n",
    "\n",
    "    - Non-Linear SVM, Feature Map and Kernel Trick\n",
    "\n",
    "    - Online SVM\n",
    "    \n",
    "    - SVM in Scikit-learn\n",
    "\n",
    "    - SVM for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Support Vector Machines - SVM</center></h1>\n",
    "\n",
    "<center><img src=\"img/svm-title.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [4]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Revisiting Binary Classification</center></h1>\n",
    "\n",
    "<center><img src=\"img/binary-classification.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Linear Separability</center></h1>\n",
    "\n",
    "<center><img src=\"img/linear-separability.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Linear Classifier</center></h1>\n",
    "\n",
    "<center><img src=\"img/linear-classifier.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>N-Dimensional Linear Classifiers - Hyperplane</center></h1>\n",
    "\n",
    "<center><img src=\"img/hyperplane.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Perceptron Algorithm</center></h1>\n",
    "\n",
    "<center><img src=\"img/perceptron-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Perceptron Algorithm</center></h1>\n",
    "\n",
    "<center><img src=\"img/perceptron-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Support Vectors</center></h1>\n",
    "\n",
    "<center><img src=\"img/svm-margin.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Margin</center></h1>\n",
    "\n",
    "<center><img src=\"img/svm-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Support Vector Machine</center></h1>\n",
    "\n",
    "<center><img src=\"img/svm-optimization.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Finding the Best Hyperplane</center></h1>\n",
    "\n",
    "<center><img src=\"img/best-w.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Finding the Best Hyperplane</center></h1>\n",
    "\n",
    "<center><img src=\"img/best-w-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM - Hard Margin vs Soft Margin Classification</center></h1>\n",
    "\n",
    "- If we strictly impose that all instances must be off the margin and on the correct side, this is called <font color=\"blue\"><b>hard margin</b></font> classification.\n",
    "\n",
    "\n",
    "- There are two main issues with hard margin classification:\n",
    "    - First, it only works if the data is linearly separable.\n",
    "    - Second, it is sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The objective is to find a good balance between keeping the margin as large as possible and limiting the margin violations (i.e., instances that end up in the middle of the street or even on the wrong side). This is called <font color=\"blue\"><b>soft margin</b></font> classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM - Math Notations and Equations</center></h1>\n",
    "\n",
    "- The smaller the weight vector <b>w</b> the larger the margin.\n",
    "\n",
    "**Equation 5-2: Linear SVM classifier prediction**\n",
    "\n",
    "$\n",
    "\\hat{y} = \\begin{cases}\n",
    " 0 & \\text{if } \\mathbf{w}^T \\mathbf{x} + b < 0, \\\\\n",
    " 1 & \\text{if } \\mathbf{w}^T \\mathbf{x} + b \\geq 0\n",
    "\\end{cases}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM - Hard Margin Linear SVM</center></h1>\n",
    "\n",
    "- If we define $t^{(i)} = -1$ for negative instances  (if $y^{(i)} = 0$) and $t^{(i)} = +1$ for positive instances (if $y^{(i)} = 1$), then we can express the optimization constraint for all instances as the following:\n",
    "\n",
    "\n",
    "**Equation 5-3: Hard margin linear SVM classifier objective**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "&\\underset{\\mathbf{w}, b}{\\operatorname{minimize}}\\quad{\\frac{1}{2}\\mathbf{w}^T \\mathbf{w}} \\\\\n",
    "&\\text{subject to} \\quad t^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\ge 1 \\quad \\text{for } i = 1, 2, \\dots, m\n",
    "\\end{split}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM - Soft Margin Linear SVM</center></h1>\n",
    "\n",
    "- To get the soft margin objective, we need to introduce a slack variable ζ(i) ≥ 0\n",
    "    - For each instance: ζ(i) measures how much the i-th instance is allowed to violate the margin.\n",
    "\n",
    "\n",
    "- We now have two conflicting objectives: make the slack variables as small as possible to reduce the margin violations, and make $\\quad{\\frac{1}{2}\\mathbf{w}^T \\mathbf{w}} $ as small as possible to increase the margin. This is where the <font color = \"blue\"><b>C hyperparameter</b></font> comes in: it allows us to define the trade‐off between these two objectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Soft Margin SVM - Slack Variables</center></h1>\n",
    "\n",
    "<center><img src=\"img/slack-variable.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Soft Margin SVM - Slack Variables</center></h1>\n",
    "\n",
    "**Equation 5-4: Soft margin linear SVM classifier objective**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "&\\underset{\\mathbf{w}, b, \\mathbf{\\zeta}}{\\operatorname{minimize}}\\quad{\\dfrac{1}{2}\\mathbf{w}^T \\mathbf{w} + C \\sum\\limits_{i=1}^m{\\zeta^{(i)}}}\\\\\n",
    "&\\text{subject to} \\quad t^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\ge 1 - \\zeta^{(i)} \\quad \\text{and} \\quad \\zeta^{(i)} \\ge 0 \\quad \\text{for } i = 1, 2, \\dots, m\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hyperparameter C - Impact on the Margin</center></h1>\n",
    "\n",
    "<center><img src=\"img/c-impact.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hyperparameter C - Infinity</center></h1>\n",
    "\n",
    "<center><img src=\"img/C-infinity.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hyperparameter C - Small Values</center></h1>\n",
    "\n",
    "<center><img src=\"img/C-small.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hyperparameter C - Bias-Variance Trade-Off</center></h1>\n",
    "\n",
    "- C determines the width of the margin and defines the trade‐off between the two objectives of SVM optimization constraints.\n",
    "\n",
    "\n",
    "- The larger the C, the narrower the margin, the fewer margin violations, (probably) the less generalized model, the more prone to overfitting,  <font color='blue'>the lower the bias, the higher the variance</font>.\n",
    "\n",
    "\n",
    "- The smaller the C, the wider the margin, the more margin violations, (probably) the more generalized model, the more prone to underfitting, <font color='blue'>the higher the bias, the lower the variance</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Optimization</center></h1>\n",
    "\n",
    "<center><img src=\"img/optimization.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Cost Function</center></h1>\n",
    "\n",
    "<center><img src=\"img/loss.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Cost Function</center></h1>\n",
    "\n",
    "**Equation 5-13: Linear SVM classifier cost function**\n",
    "\n",
    "$\n",
    "J(\\mathbf{w}, b) = \\dfrac{1}{2} \\mathbf{w}^T \\mathbf{w} \\quad + \\quad C {\\displaystyle \\sum\\limits_{i=1}^{m}max\\left(0, 1 - t^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\right)}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Cost Function</center></h1>\n",
    "\n",
    "<center><img src=\"img/loss-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/hinge.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Optimization - Finding Global Optima</center></h1>\n",
    "\n",
    "<center><img src=\"img/optimization-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convex Function</center></h1>\n",
    "\n",
    "<center><img src=\"img/convex-function.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convex Function Examples</center></h1>\n",
    "\n",
    "<center><img src=\"img/convex-examples.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Cost Function Is Convex</center></h1>\n",
    "\n",
    "<center><img src=\"img/svm-convex.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Non-Linearly Separable Data</center></h1>\n",
    "\n",
    "<center><img src=\"img/non-linear.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Transforming Non-Linear Data</center></h1>\n",
    "\n",
    "<center><img src=\"img/transform.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM in Transformed Feature Space</center></h1>\n",
    "\n",
    "<center><img src=\"img/feature-map.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Equation 5-8: Second-degree polynomial mapping**\n",
    "\n",
    "$\n",
    "\\phi\\left(\\mathbf{x}\\right) = \\phi\\left( \\begin{pmatrix}\n",
    "  x_1 \\\\\n",
    "  x_2\n",
    "\\end{pmatrix} \\right) = \\begin{pmatrix}\n",
    "  {x_1}^2 \\\\\n",
    "  \\sqrt{2} \\, x_1 x_2 \\\\\n",
    "  {x_2}^2\n",
    "\\end{pmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Kernel Trick</center></h1>\n",
    "\n",
    "- Our kernel function accepts inputs in the original lower dimensional space and returns the dot product of the transformed vectors in the higher dimensional space.\n",
    "\n",
    "- There are also theorems which guarantee the existence of such kernel functions under certain conditions.\n",
    "\n",
    "<center><img src=\"img/kernel.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Equation 5-9: Kernel trick for a 2^nd^-degree polynomial mapping**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\phi(\\mathbf{a})^T \\phi(\\mathbf{b}) & \\quad = \\begin{pmatrix}\n",
    "  {a_1}^2 \\\\\n",
    "  \\sqrt{2} \\, a_1 a_2 \\\\\n",
    "  {a_2}^2\n",
    "  \\end{pmatrix}^T \\begin{pmatrix}\n",
    "  {b_1}^2 \\\\\n",
    "  \\sqrt{2} \\, b_1 b_2 \\\\\n",
    "  {b_2}^2\n",
    "\\end{pmatrix} = {a_1}^2 {b_1}^2 + 2 a_1 b_1 a_2 b_2 + {a_2}^2 {b_2}^2 \\\\\n",
    " & \\quad = \\left( a_1 b_1 + a_2 b_2 \\right)^2 = \\left( \\begin{pmatrix}\n",
    "  a_1 \\\\\n",
    "  a_2\n",
    "\\end{pmatrix}^T \\begin{pmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2\n",
    "  \\end{pmatrix} \\right)^2 = (\\mathbf{a}^T \\mathbf{b})^2\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Kernels</center></h1>\n",
    "\n",
    "<center><img src=\"img/kernels.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Equation 5-10: Common kernels**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\text{Linear:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\mathbf{a}^T \\mathbf{b} \\\\\n",
    "\\text{Polynomial:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\left(\\gamma \\mathbf{a}^T \\mathbf{b} + r \\right)^d \\\\\n",
    "\\text{Gaussian RBF:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\exp({\\displaystyle -\\gamma \\left\\| \\mathbf{a} - \\mathbf{b} \\right\\|^2}) \\\\\n",
    "\\text{Sigmoid:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\tanh\\left(\\gamma \\mathbf{a}^T \\mathbf{b} + r\\right)\n",
    "\\end{split}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Online SVM</center></h1>\n",
    "\n",
    "\n",
    "- For linear SVM classifiers, one method for implementing an online SVM classifier is to use Gradient Descent (e.g., using `SGDClassifier`) to minimize the cost function in Equation 5-13, which is derived from the optimization problem.\n",
    "\n",
    "**Equation 5-13: Linear SVM classifier cost function**\n",
    "\n",
    "$\n",
    "J(\\mathbf{w}, b) = \\dfrac{1}{2} \\mathbf{w}^T \\mathbf{w} \\quad + \\quad C {\\displaystyle \\sum\\limits_{i=1}^{m}max\\left(0, t^{(i)} - (\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\right)}\n",
    "$\n",
    "\n",
    "\n",
    "- Usually, Gradient Descent converges slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM in Scikit-learn</center></h1>\n",
    "\n",
    "- The `LinearSVC` class is based on the `liblinear` library, which implements an optimized algorithm for linear SVMs.\n",
    "\n",
    "- It does not support the kernel trick, but it scales almost linearly with the number of training instances and the number of features. Its training time complexity is roughly $O(m × n)$.\n",
    "\n",
    "- The `LinearSVC` algorithm takes longer if you require very high precision. This is controlled by the tolerance hyperparameter `ε` (called tol in Scikit-Learn). In most classification tasks, the default tolerance is fine.\n",
    "\n",
    "- The `SGDClassifier` implements linear classifiers (SVM, logistic regression, etc.) with SGD training. By default, it uses `loss='hinge'`, which gives a linear SVM.\n",
    "\n",
    "- The `SVC` class is based on the `libsvm` library, which implements an algorithm that supports the kernel trick.\n",
    "\n",
    "- The training time complexity is usually between $O(m^2 × n)$ and $O(m^3 × n)$. Unfortunately, this means that it gets dreadfully slow when the number of training instances gets large (e.g., hundreds of thousands of instances).\n",
    "\n",
    "- This `SVC` algorithm is perfect for complex small or medium-sized training sets. It scales well with the number of features, especially with sparse features (i.e., when each instance has few nonzero features). In this case, the algorithm scales roughly with the average number of nonzero features per instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM in Scikit-learn - Computational Complexity</center></h1>\n",
    "\n",
    "<center><img src=\"img/table-5-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Table from: Ref. [1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Regression</center></h1>\n",
    "\n",
    "- The SVM algorithm is versatile: not only does it support linear and nonlinear classification, but it also supports linear and nonlinear regression.\n",
    "\n",
    "- To use SVMs for regression instead of classification, the trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible on the street while limiting margin violations (i.e., instances off the street).\n",
    "\n",
    "- The width of the street is controlled by a hyperparameter, `ε` . Figure 5-10 shows two linear SVM Regression models trained on some random linear data, one with a large margin `(ε=1.5)` and the other with a small margin `(ε=0.5)`.\n",
    "\n",
    "- Adding more training instances within the margin does not affect the model’s predictions; thus, the model is said to be *ε-insensitive*.\n",
    "\n",
    "<center><img src=\"img/fig-5-10.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Non-linear Regression</center></h1>\n",
    "\n",
    "- To tackle nonlinear regression tasks, you can use a kernelized SVM model.\n",
    "\n",
    "- Figure 5-11 shows SVM Regression on a random quadratic training set, using a second-degree polynomial kernel.\n",
    "\n",
    "- There is little regularization in the left plot (i.e., a large C value), and much more regularization in the right plot (i.e., a small C value).\n",
    "\n",
    "<center><img src=\"img/fig-5-11.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Regression in Scikit-learn</center></h1>\n",
    "\n",
    "- The following code uses Scikit-Learn’s `SVR` class (which supports the kernel trick) to produce the model represented on the left in Figure 5-11 (see the previous slide):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The `SVR` class is the regression equivalent of the `SVC` class, and the `LinearSVR` class is the regression equivalent of the `LinearSVC` class.\n",
    "\n",
    "- The `LinearSVR` class scales linearly with the size of the training set (just like the `LinearSVC` class), while the `SVR` class gets much too slow when the training set grows large (just like the `SVC` class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Summary</center></h1>\n",
    "\n",
    "- Linear Classifiers and Perceptron Algorithm\n",
    "\n",
    "- Hard Margin vs Soft Margin SVM\n",
    "\n",
    "- Slack Variables\n",
    "\n",
    "- Hyperparameter C and Bias-Varince Trade-off\n",
    "\n",
    "- SVM Optimization Problem: Constrained and Unconstrained Form\n",
    "\n",
    "- SVM Cost Function and Convex Discussion\n",
    "\n",
    "- Hinge Loss Function\n",
    "\n",
    "- Non-Linear SVM, Feature Map and Kernel Trick\n",
    "\n",
    "- Online SVM\n",
    "\n",
    "- SVM in Scikit-learn\n",
    "\n",
    "- SVM Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>References</center></h1>\n",
    "\n",
    "[1] \"Hands-On ML\" Textbook 2nd Edition Oct 2019\n",
    "\n",
    "[2] [Zisserman's SVM Slides-1 - Oxford University](http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf)\n",
    "\n",
    "[3] [Zisserman's SVM Slides-2 - Oxford University](http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf)\n",
    "\n",
    "[4] N. Leal et al \"MARINE VESSEL RECOGNITION BY ACOUSTIC SIGNATURE\" 2015\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
